{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartFlush Artificial Neural Network (ANN) Development\n",
    "\n",
    "This notebook focuses on developing and experimenting with ANN models for flush level prediction.\n",
    "\n",
    "## Contents\n",
    "1. Data Preparation\n",
    "2. ANN Architecture Design\n",
    "3. Model Training\n",
    "4. Learning Curves\n",
    "5. Hyperparameter Tuning\n",
    "6. Model Evaluation\n",
    "7. Comparison with Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_loading import load_and_combine_data, prepare_data\n",
    "from src.models import (\n",
    "    create_ann_model,\n",
    "    train_ann_model,\n",
    "    plot_learning_curves\n",
    ")\n",
    "from src.metrics import evaluate_model, plot_confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_files = [\n",
    "    '../data/Combined_Data.xlsx',\n",
    "    '../data/mon_fichier.xlsx'\n",
    "]\n",
    "\n",
    "existing_files = [f for f in data_files if Path(f).exists()]\n",
    "\n",
    "if existing_files:\n",
    "    df = load_and_combine_data(existing_files, how='concat')\n",
    "else:\n",
    "    print(\"Creating synthetic data for demonstration\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'photodiode_1': np.random.randn(n_samples),\n",
    "        'photodiode_2': np.random.randn(n_samples),\n",
    "        'waste_level': np.random.randint(1, 6, n_samples),\n",
    "        'sensor_1': np.random.randn(n_samples),\n",
    "        'sensor_2': np.random.randn(n_samples),\n",
    "        'flush_level': np.random.randint(0, 11, n_samples)  # 0-10 for 11 classes\n",
    "    })\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# Prepare data\n",
    "target_col = 'flush_level' if 'flush_level' in df.columns else df.columns[-1]\n",
    "data_dict = prepare_data(\n",
    "    df,\n",
    "    target_col=target_col,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    vif_threshold=10.0,\n",
    "    apply_standardization=True\n",
    ")\n",
    "\n",
    "X_train = data_dict['X_train']\n",
    "X_test = data_dict['X_test']\n",
    "y_train = data_dict['y_train']\n",
    "y_test = data_dict['y_test']\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ANN Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ANN architecture\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Architecture 1: Deep network with 3 hidden layers\n",
    "hidden_layers_1 = [128, 64, 32]\n",
    "\n",
    "model_1 = create_ann_model(\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    hidden_layers=hidden_layers_1,\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "print(\"Model Architecture 1:\")\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_nn.shape}\")\n",
    "print(f\"Validation set: {X_val_nn.shape}\")\n",
    "\n",
    "# Train model\n",
    "ann_model, history = train_ann_model(\n",
    "    X_train_nn, y_train_nn,\n",
    "    X_val_nn, y_val_nn,\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Analyze training history\n",
    "print(f\"\\nFinal Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different architectures\n",
    "architectures = [\n",
    "    [128, 64, 32],\n",
    "    [256, 128, 64],\n",
    "    [64, 32],\n",
    "    [128, 64]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, hidden_layers in enumerate(architectures):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Architecture {i+1}: {hidden_layers}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        model, hist = train_ann_model(\n",
    "            X_train_nn, y_train_nn,\n",
    "            X_val_nn, y_val_nn,\n",
    "            hidden_layers=hidden_layers,\n",
    "            dropout_rate=0.2,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=32,\n",
    "            epochs=50,\n",
    "            early_stopping_patience=5\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        results.append({\n",
    "            'architecture': str(hidden_layers),\n",
    "            'val_accuracy': hist.history['val_accuracy'][-1],\n",
    "            'test_accuracy': test_acc,\n",
    "            'parameters': model.count_params()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training architecture {hidden_layers}: {e}\")\n",
    "\n",
    "# Compare results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_df['val_accuracy'], width, label='Validation Accuracy', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['test_accuracy'], width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Architecture')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('ANN Architecture Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"Arch {i+1}\" for i in range(len(results_df))], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "y_pred_proba = ann_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "ann_metrics = evaluate_model(y_test, y_pred, 'ANN')\n",
    "print(\"\\nANN Performance Metrics:\")\n",
    "for metric, value in ann_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, title='ANN - Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction probabilities\n",
    "avg_max_prob = np.mean(np.max(y_pred_proba, axis=1))\n",
    "print(f\"Average maximum probability: {avg_max_prob:.4f}\")\n",
    "\n",
    "# Plot probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of maximum probabilities\n",
    "axes[0].hist(np.max(y_pred_proba, axis=1), bins=30, alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(avg_max_prob, color='red', linestyle='--', label=f'Mean: {avg_max_prob:.3f}')\n",
    "axes[0].set_xlabel('Maximum Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Maximum Prediction Probabilities')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Entropy of predictions\n",
    "from scipy.stats import entropy\n",
    "entropies = [entropy(probs) for probs in y_pred_proba]\n",
    "axes[1].hist(entropies, bins=30, alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('Prediction Entropy')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Prediction Entropy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage prediction entropy: {np.mean(entropies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Best Architecture**: [Add best performing architecture]\n",
    "2. **Performance**: [Add accuracy and other metrics]\n",
    "3. **Training Insights**: [Add observations about convergence, overfitting]\n",
    "4. **Comparison**: [Add comparison with other models]\n",
    "\n",
    "### Recommendations:\n",
    "1. [Add recommendation 1]\n",
    "2. [Add recommendation 2]\n",
    "3. [Add recommendation 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
